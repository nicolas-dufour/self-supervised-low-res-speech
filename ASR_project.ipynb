{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASR_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolas-dufour/self-unsupervised-low-res-speech/blob/master/ASR_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWmxr7ZzTUAo"
      },
      "source": [
        "# ASR_project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrYGlzvRTg5b"
      },
      "source": [
        "## Install & setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS9vGNhgwlBF"
      },
      "source": [
        "# Load Git folder\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "repo_user = 'nicolas-dufour'\n",
        "user = input('Github Username: ')\n",
        "password = getpass('Password: ')\n",
        "repo_name = 'self-unsupervised-low-res-speech'\n",
        "# your password is converted into url format\n",
        "password = urllib.parse.quote(password)\n",
        "cmd_string = 'git clone https://{0}:{1}@github.com/{2}/{3}.git'.format(user, password, repo_user, repo_name)\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable\n",
        "# Bad password fails silently so make sure the repo was copied\n",
        "assert os.path.exists(f\"/content/{repo_name}\"), \"Incorrect Password or Repo Not Found, please try again\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FihWijf6Bcnu"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyoLr2c8TfEX"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!sudo apt-get install festival espeak-ng mbrola\n",
        "!pip install torchaudio\n",
        "!pip install phonemizer\n",
        "!pip install pytorch_lightning\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21h4fmcvG9vM"
      },
      "source": [
        "%cd /content/self-unsupervised-low-res-speech/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evjrtjJ67--S"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKkVOcIdf-dB"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import urllib\n",
        "from phonemize import phonemize_labels\n",
        "from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC , Wav2Vec2FeatureExtractor\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "\n",
        "from dataloader import CommonVoiceDataModule\n",
        "from metrics import PER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYeGBMVxLhBf"
      },
      "source": [
        "## Create the data_module (instance of CommonVoiceDataModule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VZpsTg7NliC"
      },
      "source": [
        "### Take the url from https://commonvoice.mozilla.org/fr/datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQrULnFH1JwB"
      },
      "source": [
        "url = input('Url:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3u9hdjh0Jx"
      },
      "source": [
        "### Choose a language from https://github.com/espeak-ng/espeak-ng/blob/master/docs/languages.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bamhu1obLKE5"
      },
      "source": [
        "data_module = CommonVoiceDataModule(\n",
        "    url,\n",
        "    'el',\n",
        "    labels_folder=None,\n",
        "    phonemize=True,\n",
        "    label_type='phonemes',\n",
        "    batch_size= 4\n",
        "    )\n",
        "data_module.prepare_data()\n",
        "data_module.setup()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US8Ep5ifSBe5"
      },
      "source": [
        "### Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOrKfDMeIfnn"
      },
      "source": [
        "train_loader=data_module.train_dataloader()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r99ggTAaOzGo"
      },
      "source": [
        "sounds, tokens = next(iter(train_loader))\n",
        "sound = np.array(sounds[0])   #(sound,label), [idx_batch,data]\n",
        "token = tokens[0]\n",
        "freq = 16000   #Hz\n",
        "plt.plot(sound)\n",
        "print(\"raw label: {}\".format(np.array(token)))\n",
        "print(\"phonetic label: {}\".format(data_module.tokenizer.decode(np.array(token))))\n",
        "Audio(sound, rate=freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9RksVPBoPBM"
      },
      "source": [
        "## Useful functions for CTC loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esTmw2j4k8RM"
      },
      "source": [
        "def len_phoneme(phonms):\n",
        "  bs,max_length=phonms.shape\n",
        "  input_lengths = torch.zeros(bs ,dtype=torch.long)\n",
        "  for idx in range(bs):\n",
        "    input_lengths[idx]= max_length-int(sum(phonms[idx,:]==0))  #correspond to the number of non-zero labels in phonms\n",
        "  return input_lengths\n",
        "\n",
        "def len_logits(logts):\n",
        "  max_length,bs,vocab_size=logts.shape\n",
        "  if vocab_size!=48:\n",
        "    raise Exception(\"Vocab size is not consistent\")\n",
        "\n",
        "  return torch.full(size=(bs,), fill_value=max_length, dtype=torch.long)\n",
        "\n",
        "def recover_tokens(output_tokens):\n",
        "    recovered_tokens = []\n",
        "    for list_tokens in  output_tokens:\n",
        "        list_decoded = [list_tokens[0].item()]\n",
        "        j=0\n",
        "        for i in range(len(list_tokens)):\n",
        "            if list_decoded[j]!=list_tokens[i]:\n",
        "                list_decoded.append(list_tokens[i].item())\n",
        "                j+=1\n",
        "        recovered_tokens.append(list(filter(lambda a: a != 0, list_decoded)))\n",
        "    return recovered_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2WlDREXzmeX"
      },
      "source": [
        "## Construction of the CTC network with Wav2Vec2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1aGQ7GVvfi1"
      },
      "source": [
        "#with Pytorch Lightning | complete version\n",
        "class CTCNetwork(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CTCNetwork, self).__init__()\n",
        "        \n",
        "        self.phonemeSizeAlphabet=48   #the size of the phonetic alphabet being 48\n",
        "        self.criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "\n",
        "        self.feature_extractor = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "        #self.feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True).from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "        for param in self.feature_extractor.parameters():\n",
        "             param.requires_grad = False\n",
        "        self.feature_extractor.lm_head=torch.nn.Linear(in_features=768, out_features=self.phonemeSizeAlphabet, bias=True)\n",
        "\n",
        "        self.val_per = PER() \n",
        "        self.test_per = PER() \n",
        "\n",
        "    def forward(self, x_audio):\n",
        "        x_logits = self.feature_extractor(x_audio).logits.permute(1,0,2)\n",
        "        log_prob = torch.nn.functional.log_softmax(x_logits, dim=2) #logarithmized probabilities of the outputs\n",
        "        return log_prob\n",
        "\n",
        "    def relaxation(self,type_relax):\n",
        "        if type_relax==\"soft\":\n",
        "            for name,param in self.named_parameters():\n",
        "                if name.startswith('feature_extractor.wav2vec2.encoder.layers.11') or name.startswith('feature_extractor.wav2vec2.encoder.layers.10') or name.startswith('feature_extractor.lm_head'):\n",
        "                    param.requires_grad = True\n",
        "        elif type_relax==\"hard\":\n",
        "            for name,param in self.named_parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        # REQUIRED\n",
        "        x_audio, phonemes = batch\n",
        "        log_prob = self(x_audio)\n",
        "        loss = self.criterion(log_prob,phonemes,len_logits(log_prob),len_phoneme(phonemes))\n",
        "        self.log('train_loss', loss, on_epoch=True, on_step=True)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        # REQUIRED\n",
        "        x_audio, phonemes = batch\n",
        "        log_prob = self(x_audio)\n",
        "        loss = self.criterion(log_prob,phonemes,len_logits(log_prob),len_phoneme(phonemes))\n",
        "        decoded_tokens = recover_tokens(log_prob.argmax(dim=2).permute(1,0))\n",
        "        self.log('val_loss', loss, on_epoch=True, on_step=False)\n",
        "        self.val_per(decoded_tokens, phonemes)\n",
        "        return loss\n",
        "    \n",
        "    def validation_epoch_end(self, losses):\n",
        "        self.log('val_per',self.val_per.compute())\n",
        "        self.val_per.reset()\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        # REQUIRED\n",
        "        x_audio, phonemes = batch\n",
        "        log_prob = self(x_audio)\n",
        "        decoded_tokens = recover_tokens(log_prob.argmax(dim=2).permute(1,0))\n",
        "        self.test_per(decoded_tokens, phonemes)\n",
        "\n",
        "    def test_epoch_end(self, losses):\n",
        "        self.log('test_per',self.test_per.compute())\n",
        "        self.test_per.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # REQUIRED\n",
        "        # can return multiple optimizers and learning_rate schedulers\n",
        "        # (LBFGS it is automatically supported, no need for closure function)\n",
        "        return torch.optim.Adam(self.parameters(), lr=2e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAKJEA7TEoLT"
      },
      "source": [
        "model = CTCNetwork()\n",
        "model.relaxation(\"soft\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dgaqQusu_PM"
      },
      "source": [
        "wandb_logger = pl.loggers.WandbLogger(project='ASR Project')\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "     mode ='min',\n",
        "     monitor='val_per',\n",
        "     dirpath='/content/drive/MyDrive/self-supervised-speech/models',\n",
        "    filename='asr_model_wav2vec_fr-{epoch:02d}-{val_f1_score:.2f}'\n",
        ")\n",
        "trainer = pl.Trainer(\n",
        "    gpus = 1,\n",
        "    progress_bar_refresh_rate =20,\n",
        "    logger = wandb_logger,\n",
        "    callbacks=[checkpoint_callback])    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an3iYKS7mhXh"
      },
      "source": [
        "#trainer.tune(model)\n",
        "trainer.fit(model, data_module) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MzV_dZWN-hc"
      },
      "source": [
        "## Look what the model is producing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-unCW9ZOKWI"
      },
      "source": [
        "audio,label=next(iter(data_module.train_dataloader()))\n",
        "model.eval()\n",
        "pred=model(audio)\n",
        "print('pred shape: {}'.format(pred.shape))\n",
        "print('Groundtruth Phonemes:  {}'.format(data_module.tokenizer.decode(np.array(label[0]))))\n",
        "print('Phonemes produce from the model:{}'.format(data_module.tokenizer.decode(np.array(pred.argmax(dim=2)[:,0]))))\n",
        "Audio(np.array(audio[0]), rate=freq)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEvbLYAqWxJV"
      },
      "source": [
        "len_logits(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJjwdWBkaW1F"
      },
      "source": [
        "len_phoneme(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcuhjya1VLyR"
      },
      "source": [
        "## Debugging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5F29-yqVST0"
      },
      "source": [
        "#definition of CTCloss\n",
        "ctc_loss=torch.nn.CTCLoss(zero_infinity=True) \n",
        "\n",
        "#arguments\n",
        "logits=torch.ones((10,4,48)).log_softmax(2)   # logits:Log_probs      : Tensor of size (max input length  ,  batch size  ,  number of classes)\n",
        "phoneme=torch.ones((4,7))                     # phoneme : Targets        : Tensor of size (    batch size     ,          max target length  )\n",
        "lenlog=len_logits(logits)                     # lenlog: Input_lengths  : Tensor of size               (batch size )    --> indicates the input length of each sequence of the batch\n",
        "lenpho= len_phoneme(phoneme)                  # Target_lengths :Tensor of size                (batch size )    --> indicates the target length of each sequence of the batch\n",
        "\n",
        "#compute the loss\n",
        "lossDebug=ctc_loss(logits,phoneme,lenlog,lenpho)\n",
        "print(lossDebug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3niUtwzjLVL7"
      },
      "source": [
        "## Git push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY_k2V8sLYos"
      },
      "source": [
        "# Git Ignore setup\n",
        "!echo 'lightning_logs' >> .gitignore\n",
        "!echo 'wandb' >> .gitignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UAyrxAjMWiD"
      },
      "source": [
        "# Save to git\n",
        "!git config --global user.email \"nicolas.dufourn@gmail.com\"\n",
        "!git config --global user.name \"Nicolas DUFOUR\"\n",
        "!git add --all\n",
        "!git commit -m \"Added logging and checkpointing\"\n",
        "!git push "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}